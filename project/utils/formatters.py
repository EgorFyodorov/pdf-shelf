import re
from typing import Any, List

from project.database.models import File


def clean_page_title(title: str) -> str:
    """–û—á–∏—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç –ª–∏—à–Ω–∏—Ö —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤."""
    if not title:
        return ""
    
    # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã —Å–∞–π—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
    suffixes = [
        r"\s*[/\\]\s*–•–∞–±—Ä\s*$",  # " / –•–∞–±—Ä" –∏–ª–∏ " \ –•–∞–±—Ä" –≤ –∫–æ–Ω—Ü–µ
        r"\s*[-‚Äî‚Ä¢¬∑]\s*–•–∞–±—Ä.*$",  # " - –•–∞–±—Ä" –∏–ª–∏ " ‚Äî –•–∞–±—Ä"
        r"\s*[/\\]\s*Habr\s*$",
        r"\s*[-‚Äî‚Ä¢¬∑]\s*Habr.*$",
        r"\s*[/\\]\s*Medium\s*$",
        r"\s*[-‚Äî‚Ä¢¬∑]\s*Medium.*$",
        r"\s*[/\\]\s*VC\.ru\s*$",
        r"\s*[-‚Äî‚Ä¢¬∑]\s*VC\.ru.*$",
        r"\s*[/\\]\s*The Bell\s*$",
        r"\s*[-‚Äî‚Ä¢¬∑]\s*The Bell.*$",
        r"\s*\|\s*[A-Za-z–ê-–Ø–∞-—è0-9\s]+\s*$",  # " | –ù–∞–∑–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞" —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ
    ]
    
    cleaned = title
    for suffix in suffixes:
        cleaned = re.sub(suffix, "", cleaned, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    max_length = 100
    if len(cleaned) > max_length:
        cleaned = cleaned[:max_length].rsplit(" ", 1)[0] + "..."
    
    return cleaned or "–î–æ–∫—É–º–µ–Ω—Ç"


def format_analysis_card(file: File, include_url: bool = True) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."""

    analysis = file.analysis_json
    title = file.title
    source_url = file.source_url

    volume = analysis.get("volume", {})
    complexity = analysis.get("complexity", {})

    page_count = volume.get("page_count") or "?"
    byte_size = volume.get("byte_size") or 0
    size_mb = byte_size / (1024 * 1024) if byte_size else 0
    word_count = volume.get("word_count", 0)
    reading_time = volume.get("reading_time_min", 0)

    complexity_level = complexity.get("level", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")

    tags_str = ", ".join(file.tags) if file.tags else "–ë–µ–∑ —Ç–µ–≥–æ–≤"

    lines = [
        f'üìÑ "{title}"',
    ]

    if include_url and source_url:
        lines.append(source_url)
        lines.append("")

    if size_mb > 0:
        lines.append(
            f"–û–±—ä—ë–º: {page_count} —Å—Ç—Ä. ({size_mb:.1f} –ú–ë) ‚Ä¢ "
            f"{word_count} —Å–ª–æ–≤ ({reading_time:.0f} –º–∏–Ω)"
        )
    else:
        lines.append(
            f"–û–±—ä—ë–º: {page_count} —Å—Ç—Ä. ‚Ä¢ " f"{word_count} —Å–ª–æ–≤ ({reading_time:.0f} –º–∏–Ω)"
        )

    lines.append(f"–°–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity_level}")
    lines.append(f"–¢–µ–º—ã: {tags_str}")

    return "\n".join(lines)


def extract_urls(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö URL –∏–∑ —Ç–µ–∫—Å—Ç–∞."""

    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    urls = re.findall(url_pattern, text)

    return list(set(urls))


def extract_tags_from_analysis(analysis_json: dict[str, Any]) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ analysis_json (–∏–∑ category.label)."""

    tags = []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ category.label
    category = analysis_json.get("category", {})
    category_label = category.get("label")
    if category_label:
        tags.append(category_label)

    return tags


def format_multiple_files_summary(
    files: List[tuple[str, float, str, str]], total_time: float
) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤.

    Args:
        files: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (url, reading_time_min, main_topic, complexity_level)
        total_time: –æ–±—â–µ–µ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
    """

    count = len(files)

    lines = [
        (
            f"‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ {count} –º–∞—Ç–µ—Ä–∏–∞–ª–∞:"
            if count < 5
            else f"‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ {count} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤:"
        ),
        "",
    ]

    for idx, (url, time, topic, complexity) in enumerate(files, 1):
        domain = url.split("//")[-1].split("/")[0] if url else "unknown"
        lines.append(f"{idx}. {domain} ({time:.0f} –º–∏–Ω, {complexity}) ‚Äî {topic}")

    lines.append("")
    lines.append(f"–í—Å–µ–≥–æ: {total_time:.0f} –º–∏–Ω—É—Ç —á—Ç–µ–Ω–∏—è")

    return "\n".join(lines)


def format_file_list_for_export(files: List[File], total_time: float) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ (–∏—Ç–æ–≥–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ).

    Args:
        files: —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ File
        total_time: –æ–±—â–µ–µ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
    """

    count = len(files)

    return (
        f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {count} –º–∞—Ç–µ—Ä–∏–∞–ª–∞ ({total_time:.0f} –º–∏–Ω—É—Ç —á—Ç–µ–Ω–∏—è)"
        if count < 5
        else f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {count} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ ({total_time:.0f} –º–∏–Ω—É—Ç —á—Ç–µ–Ω–∏—è)"
    )
